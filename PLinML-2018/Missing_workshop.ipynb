{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Missing_workshop.ipynb","version":"0.3.2","provenance":[{"file_id":"1kt0sRMefFamN7a_PhMeVmod4j6xGmD7p","timestamp":1545032328543},{"file_id":"1gqLBFPgaOUR49VR52FuoDtc9_p2PDb4c","timestamp":1535996979732}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"y7iidWQaZTA5","colab_type":"text"},"cell_type":"markdown","source":["\n","![alt text](http://gmum.net/images/logo.jpg =300x150)\n","\n","\n","---\n","\n","\n","# Workshop Overview: \n","\n","\n","The workshop goal is to introduce a simple auto-encoder model and present solutions for dealing with missing data, including\n","the recently proposed method in (https://papers.nips.cc/paper/7537-processing-of-missing-data-by-neural-networks.pdf)  \n","\n","\n","\n","1.   Read MNIST data\n","2.   Implement Classical auto-encoder\n","3.   Download MNIST images with missing values\n","4.   Inpaint missing values and feed them to an auto-encoder\n","5.   Our method\n","6.   MNIST classification\n","\n","\n","---\n","\n","\n","\n","\n"]},{"metadata":{"id":"BPnNfHcSZBrR","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import sys\n","from tensorflow.keras.layers import Dense\n","\n","\n","%matplotlib inline\n","\n","transform_write = {'blue': '\\033[94m', 'green': '\\033[92m', 'yellow': '\\033[93m', 'red': '\\033[91m', 'bold': '\\033[1m', 'italic': '\\x1B[3m', 'underline': '\\033[4m', 'end': '\\033[0m'}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mAFeD6_G4BP0","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","### 1. Read MNIST data\n","Read the MNIST dataset. Preprocess it and reshape into vectors of dimension  `784`. \n","\n","\n","---  \n"]},{"metadata":{"id":"QTUONu_rqzne","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import data\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","# set the data to be in interval [0,1].\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","valid_length = len(y_test)//2;\n","x_valid = x_test[:valid_length]\n","y_valid = y_test[:valid_length]\n","x_test = x_test[valid_length:]\n","y_test = y_test[valid_length:]\n","\n","\n","print('Shape of train data: {}'.format(x_train.shape))\n","print('Shape of train labels: {}'.format(y_train.shape))\n","print('Shape of valid data: {}'.format(x_valid.shape))\n","print('Shape of valid labels: {}'.format(y_valid.shape))\n","print('Shape of test data: {}'.format(x_test.shape))\n","print('Shape of test labels: {}'.format(y_test.shape))\n","\n","# resize to 784\n","x_train = x_train.reshape(-1, 784)\n","x_valid = x_valid.reshape(-1, 784)\n","x_test = x_test.reshape(-1, 784)\n","\n","# change background to white\n","x_train = 1. - x_train\n","x_test = 1. - x_test\n","x_valid = 1. - x_valid"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IN30q084AY3_","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","### 2. Classical auto-encoder\n","\n","Build and train a classical auto-encoder model, as described by the hyperparameters and image below:\n","![](https://ww2.ii.uj.edu.pl/~z1101353/ae.png)\n","\n","\n","---\n","\n"]},{"metadata":{"id":"LtvfhWaXczxA","colab_type":"text"},"cell_type":"markdown","source":["2.1. Set up the hyperparameters"]},{"metadata":{"id":"RJTbtId1csuk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training Parameters\n","learning_rate = 0.01\n","n_epochs = 50\n","batch_size = 64\n","\n","# Network Parameters\n","num_hidden_1 = 256  # 1st layer num features\n","num_hidden_2 = 128  # 2nd layer num features \n","num_hidden_3 = 64  # 3nd layer num features (the latent dim)\n","num_input = 784  # MNIST data input (img shape: 28*28)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xdAqz8EQdFKL","colab_type":"text"},"cell_type":"markdown","source":["2.2. Build the auto-encoder model graph, as described by the hyperparameters and image above. The architecture is a feed-forward network.\n","\n","\n","*   Ex 1. : Complete the code for the encoder. \n","*   Ex 2. : Complete the code for the decoder\n","*   Ex 3. : Define the prediction. Be sure to pass the output of the encoder as the input to decoder.\n","*   Ex 4. : Define the loss (MSE) and the optimization operation.   Use the [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). Be sure to pass the `learning_rate` to the optimizer's constructor.\n","\n","\n"]},{"metadata":{"id":"3naeJy1D9Fsx","colab_type":"code","colab":{}},"cell_type":"code","source":["# tf Graph input (only pictures)\n","X = tf.placeholder(\"float\", [None, num_input])\n","\n","def encoder(x):\n","    # Ex 1. : Complete the code for the encoder.\n","    ???\n","\n","def decoder(x):\n","    # Ex 2. : Complete the code for the decoder.\n","    ???\n","    \n","# Ex. 3. : Define the prediction\n","encoded = ???\n","decoded = ???\n","\n","\n","# Ex. 4. : Define the loss and optimization operation\n","loss = ??\n","\n","optimizer_op = ??\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ekyFywq4f9bk","colab_type":"text"},"cell_type":"markdown","source":["2.3.  Train the model\n","\n","*   Ex.5: Define the train function for the model, with arguments:\n"," *   `sess` -  the open tf.Session. Assume the global variables have already been initialized.\n"," *   `x_train` - the train dataset.\n"," *   `x_valid` - the validation dataset. After each completed epoch you may evaluate and report the loss on the validation dataset, as a way to monitor the model performence on out-of-train data. \n"," *   `batch_size` - the size of one batch.\n"," *   `n_epochs` - number of epochs \n","\n","After each epoch remember to shuffle the train dataset. \n","\n","If you want, you may use [`tqdm`](https://github.com/tqdm/tqdm) function for a nice progress bar, as in section 2.4. \n","\n","\n","\n","\n"]},{"metadata":{"id":"4KpgchA-jqJj","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(sess, x_train, x_valid, batch_size, n_epochs):\n","    #Ex. 5: Define the train function\n","    ???\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"KFoqdWHugGTh","colab_type":"text"},"cell_type":"markdown","source":["2.4. Evaluate the model.\n","\n","The function below takes a tf.Session with a trained model, and runs the evaluation on test data."]},{"metadata":{"id":"l6-p895bf3H7","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(sess, x_test, batch_size):\n","    # calculate MSE between original test data and reconstruction of test data\n","    mse = 0.\n","    n_batches = x_test.shape[0] // batch_size\n","    for iteration in tqdm(range(n_batches)):\n","        batch_x = x_test[iteration * batch_size : (iteration+1)*batch_size, :]\n","        reconstruction = sess.run(decoded, feed_dict={X: batch_x})\n","        mse += np.mean(np.square(batch_x - reconstruction))\n","    mse = mse / n_batches\n","\n","    print('Number of samples of test data: {:d}'.format(x_test.shape[0]))\n","    print('{}MSE between original test data and reconstruction of test data equals {}{:0.5f}{}'.format(transform_write['italic'], transform_write['bold'], mse, transform_write['end']))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I-2A6FPWk5Bj","colab_type":"text"},"cell_type":"markdown","source":["2.5. Print the samples\n","\n","The function below takes a tf.Session with a trained model, and samples random examples from test data, outputting the true and reconstructed images"]},{"metadata":{"id":"u5uW8oAjkQQL","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_samples(sess, x_test):\n","    number_of_examples = 5  \n","    batch_x = x_test[np.random.choice(x_test.shape[0], number_of_examples, replace=False), :]\n","    reconstruction = sess.run(decoded, feed_dict={X: batch_x})\n","\n","    # Display images\n","    for j in range(number_of_examples):\n","        # Draw the original digits\n","        _, ax = plt.subplots(1, 2, figsize=(4, 8))\n","        ax[0].imshow(batch_x[j].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[0].axis('off')\n","        ax[0].set_title('Original')\n","\n","        # Draw the reconstruction digits\n","        ax[1].imshow(reconstruction[j].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[1].axis('off')\n","        ax[1].set_title('Reconstruction')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fq1TwTjJk98m","colab_type":"text"},"cell_type":"markdown","source":["2.6. Run the code\n","\n","Create a tf.Session and run the functions:"]},{"metadata":{"id":"LGsuj78XkEZq","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get the global varaibles initializer\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    # initialize the varaibles\n","    sess.run(init)\n","    # train\n","    train(sess, np.copy(x_train), x_valid, batch_size, n_epochs)\n","    # evaluate\n","    test(sess, x_test, batch_size)\n","    # display samples from test\n","    print_samples(sess, x_test)\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"JEx_yvAxCZC6","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","### 3. Download missing MNIST images\n","Prepare the missing images. If you have already downloaded the data, you can use the code below to upload it. If you would like to upload the .zip file, rememebr to unpack it using:  \n","`!unzip -o missing.zip`\n","\n","\n","\n","\n","---\n","\n"]},{"metadata":{"id":"67YysSusDBIZ","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JT8vn6w8DlrU","colab_type":"text"},"cell_type":"markdown","source":["If you haven't downloaded the data yet, you may do it now, or just run the cell below (do whatever will be faster for you)"]},{"metadata":{"id":"j9gHvwu2DkFX","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download the data with missing values\n","!wget -nc https://ww2.ii.uj.edu.pl/~z1101353/missing.zip\n","!unzip -o missing.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rNNtys2qDzPL","colab_type":"text"},"cell_type":"markdown","source":["Now load the data from the files. "]},{"metadata":{"id":"XwzdSLV0yPOt","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load the data\n","train_data = np.load(\"missing_train.npy\")\n","valid_data = np.load(\"missing_valid.npy\")\n","test_data = np.load(\"missing_test.npy\")\n","\n","number_of_examples = 5\n","\n","# Display images\n","for i in range(number_of_examples):\n","    _, ax = plt.subplots(1, 2, figsize=(4, 8))\n","    ax[0].imshow(x_train[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","    ax[0].axis('off')\n","    ax[0].set_title(\"Original\")\n","    \n","    ax[1].imshow(train_data[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","    ax[1].axis('off')\n","    ax[1].set_title(\"Missing\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_g5lEBWgCQQt","colab_type":"text"},"cell_type":"markdown","source":["### 4. Inpaint missing values and feed them to Auto-Encoder\n","\n","**Problem description:**\n","\n","Assume you have a dataset which lacks some values in the attributes. Consider for example the above MNSIT dataset with removed square parts. How to process such data through a network? \n","\n","A possible solution to this problem is imputing the missing values. This can be done either using simple heuristics: i.e. by replacing the NaN values with a constant, or with the median/mean of the given attribute. Other approach is to build a network that directly predicts the missing values (note, however,  that this requires the access to full, non-missing data for training). \n","\n","In this exercise you will be asked to solve the above problem with an auto-encoder. We will impute the NaNs with the attribute mean and feed them to the model. A possible intuition is that minimizing the reconstruction loss against different examples of the same digit may help the auto-encoder to place them “near” each other in the latent dim regardless the missing parts, and decode them to a better representation. \n","\n","\n","---\n","\n","\n"]},{"metadata":{"id":"cTQsCUemcVCH","colab_type":"text"},"cell_type":"markdown","source":["4.1. Set up parameters"]},{"metadata":{"id":"hDmuOrlfccYU","colab_type":"code","colab":{}},"cell_type":"code","source":["#Training Parameters\n","learning_rate = 0.01\n","n_epochs = 50\n","batch_size = 64\n","\n","# Network Parameters\n","num_hidden_1 = 256  # 1st layer num features\n","num_hidden_2 = 128  # 256 # 2nd layer num features (the latent dim)\n","num_hidden_3 = 64  # 128 # 3nd layer num features (the latent dim)\n","num_input = 784  # MNIST data input (img shape: 28*28)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9p4D4qa1ciVm","colab_type":"text"},"cell_type":"markdown","source":["4.2.  Create the model\n","\n","*   Ex. 6: Define the model and the prediction. You may use the encoder and decoder functions from Ex.1. and Ex.2. \n","*   Ex. 7: Define the loss (MSE between prediction and X_orig) and the optimization operation. Again, use [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). Be sure to consider only the non-missing point's in the calculation of the loss function. You may find [`tf.is_nan`](https://www.tensorflow.org/api_docs/python/tf/debugging/is_nan) and [`tf.where`](https://www.tensorflow.org/api_docs/python/tf/where) useful for this task. \n","*  Ex. 8: Create and fit the [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) model from `sklearn` package. Missing values are indicated as `NaN`. Use the `mean` strategy.\n"]},{"metadata":{"id":"sQtc0qTQCWli","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.impute import SimpleImputer  # used to input missing values\n","\n","# tf Graph input (only pictures).\n","# X is the placeholder for the input data, which will be inpainted using \n","# a classical method from sklearn, before being fed to the model.  \n","X = tf.placeholder(\"float\", [None, num_input])\n","X_orig = tf.placeholder(\"float\", [None, num_input])\n","\n","# Ex. 6: Define the model and the prediction\n","encoded = encoder(X)\n","decoded = decoder(encoded)\n","y_pred = decoded\n","y_true = X_orig\n","\n","\n","# Ex. 7: Define the loss and the optimization operation\n","# Get the position where the values are missing\n","# set them to zeros\n","where_isnan = tf.is_nan(y_true)\n","y_pred = tf.where(where_isnan, tf.zeros_like(y_pred), y_pred)\n","y_true = tf.where(where_isnan, tf.zeros_like(y_true), y_true)\n","\n","# Define loss and optimizer, minimize the mean square error\n","loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n","optimizer_op = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n","\n","# Ex. 8: Create and fit the imputer\n","imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","imp.fit(train_data)\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PNJ-ZJPgc89V","colab_type":"text"},"cell_type":"markdown","source":["4.3. Train the model\n","\n","*   Ex. 9: Define the train function for the model, with arguments:\n"," *   `sess` -  the open tf.Session. Assume the global variables have already been initialized.\n"," *   `x_train` - the train dataset.\n"," *   `x_valid` - the validation dataset. After each completed epoch you may evaluate and report the loss on the validation dataset, as a way to monitor the model performence on out-of-train data. \n"," *   `batch_size` - the size of one batch.\n"," *   `n_epochs` - number of epochs \n"," *   `imp` - the imputer model from `sklearn`. Use the `transform` method to input the values.\n"," \n"," Note that this function will be almost identical to the one from Ex. 5. The only difference is that you have to impute the values of input data using the  Imputer `imp` befor feeding them to the network. "]},{"metadata":{"id":"2kPQ_H5VdAF4","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(sess, train_data, valid_data, batch_size, n_epochs, imp):\n","    #Ex. 9: Create the train function\n","    ???\n","   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y1eQojlIdpfz","colab_type":"text"},"cell_type":"markdown","source":["4.4. Evaluate the model\n","\n","This function takes a tf.Session with a trained model, a dataset of non-missing values (x_test), the corresponding missing test dataset (test_data), the batch size and the imputer `imp`. It calculates the reconstruction (MSE) error between the model outputs and the true (non-missing) images."]},{"metadata":{"id":"pnEyCtSkepIb","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(sess, x_test, test_data, batch_size, imp):\n","    assert len(x_test)==len(test_data)\n","    # calculate MSE between original test data and reconstruction of test data\n","    mse = 0.\n","    mse_inside_nan = 0\n","    # test\n","    n_batches = test_data.shape[0] // batch_size\n","    for iteration in tqdm(range(n_batches)):\n","        batch_x = test_data[iteration * batch_size : (iteration+1)*batch_size, :]\n","        batch_orig = x_test[iteration * batch_size : (iteration+1)*batch_size, :]\n","        isfinite = np.isfinite(batch_x)\n","        batch_x = imp.transform(batch_x)\n","        \n","        reconstruction = sess.run(decoded, feed_dict={X: batch_x})\n","        mse += np.mean(np.square(batch_orig - reconstruction))\n","        batch_orig[isfinite] = 0.\n","        reconstruction[isfinite] = 0.\n","        mse_inside_nan += np.mean(np.square(batch_orig - reconstruction))\n","        \n","    mse = mse / n_batches\n","    mse_inside_nan = mse_inside_nan / n_batches\n","    print('Number of samples of test data: {:d}'.format(x_test.shape[0]))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['bold'], mse, transform_write['end']))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data on the area where the values were missing: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['red'] + transform_write['bold'], mse_inside_nan, transform_write['end']))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data on the area where the values were not missing: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['blue'] + transform_write['bold'], mse - mse_inside_nan, transform_write['end']))\n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yGujB1VZf4In","colab_type":"text"},"cell_type":"markdown","source":["4.5. Print the samples.\n","\n","This function takes a tf.Session and outputs samples from the non-missing test dataset, the corresponding missing images, the result of imputation with the mean and the reconstractions obtained by the trained model. "]},{"metadata":{"id":"zS5mhsq1f59y","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_samples(sess, x_test, test_data, batch_size, imp):\n","    number_of_samples = 5  \n","    # MNIST test set\n","    idx = np.random.choice(test_data.shape[0], number_of_samples, replace=False)\n","    \n","    # original image\n","    batch_orig = x_test[idx]\n","    # image with missing values\n","    batch_x = test_data[idx, :]\n","    # image with filled with mean\n","    batch_x_input = imp.transform(batch_x)\n","    # reconstructed image\n","    reconstruction = sess.run(decoded, feed_dict={X: batch_x_input})\n","    \n","\n","    # Display images\n","    for i in range(number_of_samples):\n","        # Draw the original digits\n","        _, ax = plt.subplots(1, 4, figsize=(4, 16))\n","        ax[0].imshow(batch_orig[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[0].axis('off')\n","        ax[0].set_title(\"Original\")\n","        \n","        # Draw the digits with missing \n","        ax[1].imshow(batch_x[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[1].axis('off')\n","        ax[1].set_title(\"Missing\")\n","        \n","        # Draw the input mean/knn digits\n","        ax[2].imshow(batch_x_input[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[2].axis('off')\n","        ax[2].set_title(\"Imputed using mean\")\n","            \n","        # Draw the reconstruction digits\n","        ax[3].imshow(reconstruction[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[3].axis('off')\n","        ax[3].set_title(\"Reconstructed\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_crMNEckfjGt","colab_type":"text"},"cell_type":"markdown","source":["4.6. Run the model\n","\n","Run the functions:"]},{"metadata":{"id":"4nT3eb3wcSBq","colab_type":"code","colab":{}},"cell_type":"code","source":["# Initialize the variables\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    # Run the initializer\n","    sess.run(init)\n","    train(sess, np.copy(train_data), valid_data, batch_size, n_epochs, imp)\n","    test(sess, np.copy(x_test), test_data, batch_size, imp)\n","    print_samples(sess, x_test, test_data, batch_size, imp)\n","    \n","\n","                  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"-FT_1JsIkO8N","colab_type":"text"},"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","$\\newcommand{\\R}[]{\\mathbb{R}}$\n","\n","### 5. Simple example of our method\n","\n","Problem description (Recall from the lecture):\n","\n","Let’s go back to the question how to process missing data through a network? The method proposed in https://papers.nips.cc/paper/7537-processing-of-missing-data-by-neural-networks modifies only the first layer of the network, by replacing the typical neuron’s response by its expected value. \n","\n","Assume that the missing values come from a (joint) probability distribution $F$.  For a given sample $[x, J]$ (where $x \\in \\R^D$, and  $J \\subset \\{1... D\\}$ indicates the indicies where the values are misssing), we can define  an affine subspace $S = x + span(e_J).$  \n","\n","\n","---\n","\n","\n","\n","**Example:**\n","\n","$D=2$.  A missing point $[x, \\{2\\}]$ (which simply means $x = [x_1, NaN]$) defines a subspace $S$, which is a vertical line passing through $x_1$.\n","\n","\n","---\n","\n","\n","\n","We can restrict the joint distribution $F$ to the affine subspace $S$, obtaining the (marginal) distribution $F_S(X)$:\n","<br>\n","<br>\n","\\begin{equation}\n","\\tag{1}\n","F_S(x) := \\left\\{\n","\t\\begin{array}{ll}\n","\t\t\\frac{F(x)}{\\int_S{F(s)ds}}  &  x \\in S \\\\\n","\t\t0 & x \\notin S\n","\t\\end{array}\n","\\right.\n","\\end{equation}\n","\n","<br>\n","<br>\n","\n","Now, for given $[x,J]$ we can compute the expected response of the first layer on $x \\in S$ and pass it to the subsequent layers. The remaining network architecture does not change. \n","\n","\n","\n","---\n","\n","\n","\n","The only question is, how to we get $F$ and $F_S$ and how to compute the expected response? \n","\n","Assume the following:\n","<br>\n","<br>\n","\\begin{equation}\n","\\ \\tag{2}\n","F(x) := \\sum_i{p_iN(m_i,\\Sigma_i)}\n","\\ \n","\\end{equation}\n","<br>\n","<br>\n","\n","This means that $F$ can be represented as a mixture of Gaussians distributions. All $p_i$, $m_i$ and $\\Sigma_i$ are trained together with the model weights. For such $F$ the $F_S$ can be represented as:\n","<br>\n","<br>\n","\\begin{equation}\n","\\ \\tag{3}\n","F_S(x) := \\sum_i{p_iN(m_{i,S},\\Sigma_{i,S})},\n","\\ \n","\\end{equation} \n","\\begin{equation}\n","\\ \n","m_{i,S} := [x_{J'}, m_{i,J}] \\\\\n","\\Sigma_{i,S} := [0_{\\{J',J'\\}}, \\Sigma_{i,\\{J,J\\}}]\n","\\ \n","\\end{equation} \n","where $J'= \\{1..D\\} -J$\n","<br>\n","<br>\n","\n","Using the above, the expected ReLU activation of a neuron $x$, can be computed as:\n"," \n","<br>\n","<br>\n","\\begin{equation}\n","E(ReLU_{w,b}(F_S)) =  \\sum_i{p_i,NR(\\frac{w^Tm_{i,S}+b}{\\sqrt{w^T\\Sigma_{i,S}w}})} \n","\\tag{4}\n","\\end{equation}\n","<br>\n","<br>\n","with $NR(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp{\\frac{-z^2}{2}}+\\frac{z}{2}(1+erf{\\frac{z}{\\sqrt{2}}})$\n","\n","**Therefore the goal of the first layer is to compute (4) for each sample. **\n","\n","---\n","\n"]},{"metadata":{"id":"BkBkjmI8gtnE","colab_type":"text"},"cell_type":"markdown","source":["5.1. Set up the hyperparameters"]},{"metadata":{"id":"7aSd9A4ogwSw","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","from sklearn.mixture import GaussianMixture\n","\n","# Training Parameters\n","learning_rate = 0.01\n","n_epochs = 50\n","batch_size = 64\n","\n","# Network Parameters\n","num_hidden_1 = 256  # 1st layer num features\n","num_hidden_2 = 128  # 256 # 2nd layer num features (the latent dim)\n","num_hidden_3 = 64  # 128 # 3nd layer num features (the latent dim)\n","num_input = 784  # MNIST data input (img shape: 28*28)\n","\n","n_distribution = 3  # number of Gaussian distributions\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vZWLof2Rvt35","colab_type":"text"},"cell_type":"markdown","source":["5.2  Ex. 10: Define the $NR$ function:  \n"," $NR(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp{\\frac{-z^2}{2}}+\\frac{z}{2}(1+erf{\\frac{z}{\\sqrt{2}}})$\n","\n"]},{"metadata":{"id":"t5hOGux5vtMp","colab_type":"code","colab":{}},"cell_type":"code","source":["#Ex. 10: the NR function\n","def nr(w):\n","    ???"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-95a5lr-cO-t","colab_type":"text"},"cell_type":"markdown","source":["5.3. Write the code for the first layer\n","\n","Ex. 11: Write the missing parts of the function `first_layer`, where:\n","\n","\n","*   `x` is the placeholder for inputs\n","*   `means` are the means  ($m_i$) of the Gaussians in `F` density - recall (2)\n","*   `covs` are the covariances ($\\Sigma_i$) of the Gaussians in `F` density - recall (2)\n","*    `p` are the weights of the Gaussian Mixture - recall(2).   \n","All `means`, `covs`, `p` are `tf.Variables` and will be trained jointly with other model parameters. Assume that the covarainces are diagonal.\n","\n","The function divides the batch into non-missing and missing sets (lines 8-11). The full data can be processed as usual (line 14). You are asked to complete the implementation of the computation of the expected value:\n","\\begin{equation}\n","E(ReLU_{w,b}(F_S)) =  \\sum_i{p_i,NR(\\frac{w^Tm_{i,S}+b}{\\sqrt{w^T\\Sigma_{i,S}w}})} \n","\\end{equation}\n","\n","and store it in the variable `layer_1_miss` (introduced in line 17).\n","\n","The weights $w$ are stored in columns of weight matrix `weights`.  \n","The biases $b$ are stored in vector `bias`.  \n","The $m_{i,S}$ and $\\Sigma_{i,S}$ variables are stored in `miss_mean` and `miss_cov`  in the loop.\n","\n","\n"]},{"metadata":{"id":"a7Ahf9TAcVfN","colab_type":"code","colab":{}},"cell_type":"code","source":["def first_layer(x, means, covs, p):\n","    # create the model weights and bias\n","    initializer = tf.contrib.layers.variance_scaling_initializer()\n","    weights = tf.Variable(initializer([num_input, num_hidden_1]))\n","    bias =  tf.Variable(tf.random_normal([num_hidden_1]))\n","    \n","    # Divide the batch into non-missing (x) and missing (x_miss) data\n","    check_isnan = tf.is_nan(x)\n","    check_isnan = tf.reduce_sum(tf.cast(check_isnan, tf.int32), 1)\n","    x_miss = tf.gather(x, tf.reshape(tf.where(check_isnan > 0), [-1]))  # data with missing values\n","    x = tf.gather(x, tf.reshape(tf.where(tf.equal(check_isnan, 0)), [-1]))  # data without missing values\n","\n","    # data without missing\n","    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights), bias))\n","\n","    # data with missing\n","    layer_1_miss = None\n","    where_isnan = tf.is_nan(x_miss)\n","    where_isfinite = tf.is_finite(x_miss)\n","    size = tf.shape(x_miss)\n","    \n","    \n","    weights2 = tf.square(weights)\n","    for i in range(0, n_distribution):\n","        # Ex. 11: Complete the function\n","        ???\n","\n","    layer_1 = tf.concat((layer_1, layer_1_miss), axis=0)\n","    return layer_1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ga0EX22gx_q","colab_type":"text"},"cell_type":"markdown","source":["5.4. Set up the model.  \n","The code below set-up's the model for training.  \n","Ex. 12: Complete the code for the encoder. Note that we replace the first layer by the outout of the function from section 5.3. \n","The remaining code for the auto-encoder is unchanged. Complete the code for setting up the model, loss and optimization operation. "]},{"metadata":{"id":"_x7gA_DGH8Wn","colab_type":"code","colab":{}},"cell_type":"code","source":["# tf Graph input (only pictures)\n","X = tf.placeholder(\"float\", [None, num_input])\n","\n","\n","# Building the encoder\n","def encoder(x, means, covs, p):\n","    # means, covs - means and covs of the GMM \n","    covs = tf.abs(covs)\n","    p = tf.abs(p)\n","    p = tf.div(p, tf.reduce_sum(p, axis=0))\n","\n","    # Ex. 12: Complete\n","    ???\n","\n","\n","def prep_x(x):\n","    check_isnan = tf.is_nan(x)\n","    check_isnan = tf.reduce_sum(tf.cast(check_isnan, tf.int32), 1)\n","\n","    x_miss = tf.gather(x, tf.reshape(tf.where(check_isnan > 0), [-1]))\n","    x = tf.gather(x, tf.reshape(tf.where(tf.equal(check_isnan, 0)), [-1]))\n","    return tf.concat((x, x_miss), axis=0)\n","\n","# imput the data with with the means, to get the initial parameteres for the F distribution\n","imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n","data = imp.fit_transform(train_data)\n","gmm = GaussianMixture(n_components=n_distribution, covariance_type='diag').fit(data)\n","\n","p = tf.Variable(initial_value=gmm.weights_.reshape((-1, 1)), dtype=tf.float32)\n","means = tf.Variable(initial_value=gmm.means_, dtype=tf.float32)\n","covs = tf.Variable(initial_value=gmm.covariances_, dtype=tf.float32)\n","del data, gmm, imp\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kzrKmIOMkWUI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Construct model\n","encoded = ???\n","decoded = ???\n","\n","y_pred = decoded  # prediction\n","y_true = prep_x(X)\n","\n","???\n","\n","# Define loss and optimizer operation, minimize the mean square error\n","loss = ???\n","optimizer_op = ???\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_fUVS2UbivRm","colab_type":"text"},"cell_type":"markdown","source":["5.5. Train the model"]},{"metadata":{"id":"Y3jZJV0DiujR","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(sess, train_data, valid_data, batch_size, n_epochs):\n","    for epoch in range(1, n_epochs + 1):\n","        #train\n","        n_batches = train_data.shape[0] // batch_size\n","        train_loss = []\n","        for iteration in tqdm(range(n_batches)):\n","            # get the train batch\n","            batch_x = train_data[iteration * batch_size : (iteration+1) * batch_size, :]\n","            _, l = sess.run([optimizer_op, loss],  feed_dict={X: batch_x})\n","            train_loss.append(l)\n","            \n","        #valid\n","        valid_n_batches = valid_data.shape[0]//batch_size\n","        valid_loss = []\n","        for iteration in range(valid_n_batches):\n","            batch_valid_x = valid_data[iteration * batch_size : (iteration+1) * batch_size, :]\n","            valid_loss.append(sess.run(loss,  feed_dict={X:batch_valid_x}))\n","        print('Step %i: Loss mean: %.8f Valid loss mean: %.8f' % (epoch, np.mean(train_loss), np.mean(valid_loss)))\n","        sys.stdout.flush()\n","        #shuffle the train dataset\n","        np.random.shuffle(train_data)\n","    print('\\n==========================')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hSABtjW5jdWD","colab_type":"text"},"cell_type":"markdown","source":["5.6 Evaluate the model."]},{"metadata":{"id":"EiZZHnLwjfay","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(sess, x_test, test_data, batch_size):\n","    assert len(x_test)==len(test_data)\n","    # calculate MSE between original test data and reconstruction of test data\n","    mse = 0.\n","    mse_inside_nan = 0\n","    # test\n","    n_batches = test_data.shape[0] // batch_size\n","    for iteration in tqdm(range(n_batches)):\n","        #print(\"\\r{}% \".format(100 * (iteration + 1) // n_batches), end=\"\")\n","        batch_x = test_data[iteration * batch_size : (iteration+1)*batch_size, :]\n","        batch_orig = x_test[iteration * batch_size : (iteration+1)*batch_size, :]\n","        isfinite = np.isfinite(batch_x)\n","        reconstruction = sess.run(decoded, feed_dict={X: batch_x})\n","        \n","        mse += np.mean(np.square(batch_orig - reconstruction))\n","        batch_orig[isfinite] = 0.\n","        reconstruction[isfinite] = 0.\n","        mse_inside_nan += np.mean(np.square(batch_orig - reconstruction))\n","        \n","    mse = mse / n_batches\n","    mse_inside_nan = mse_inside_nan / n_batches\n","    print('Number of samples of test data: {:d}'.format(x_test.shape[0]))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['bold'], mse, transform_write['end']))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data on the area where the values were missing: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['red'] + transform_write['bold'], mse_inside_nan, transform_write['end']))\n","    print('{}MSE between full test images and the reconstruction obtained from the missing data on the area where the values were not missing: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['blue'] + transform_write['bold'], mse - mse_inside_nan, transform_write['end']))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i0LiBIzg0y_5","colab_type":"text"},"cell_type":"markdown","source":["5.7 Print the samples."]},{"metadata":{"id":"Z2Bm3wq00zfa","colab_type":"code","colab":{}},"cell_type":"code","source":["def print_samples(sess, x_test, test_data, batch_size):\n","    number_of_examples = 5  \n","    # MNIST test set\n","    idx = np.random.choice(test_data.shape[0], number_of_examples, replace=False)\n","    batch_orig = x_test[idx]\n","    batch_x = test_data[idx, :]\n","    reconstruction = sess.run(decoded, feed_dict={X: batch_x})\n","    \n","    # Display images\n","    for i in range(number_of_examples):\n","        # Draw the original digits\n","        _, ax = plt.subplots(1, 3, figsize=(4, 12))\n","        ax[0].imshow(batch_orig[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[0].axis('off')\n","        \n","        # Draw the digits with missing \n","        ax[1].imshow(batch_x[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[1].axis('off')\n","        \n","        # Draw the reconstruction digits\n","        ax[2].imshow(reconstruction[i].reshape([28, 28]), origin=\"upper\", cmap=\"gray\")\n","        ax[2].axis('off')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LZFRBXzq07mE","colab_type":"text"},"cell_type":"markdown","source":["5.8. Run the code."]},{"metadata":{"id":"IWSpTvdvhks9","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","with tf.Session() as sess:\n","    sess.run(init)  # run the initializer\n","    train(sess, np.copy(train_data),valid_data,batch_size, n_epochs)\n","    test(sess,np.copy(x_test),test_data,batch_size)\n","    print_samples(sess, x_test, test_data,batch_size)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oEbzW0tvhS4N","colab_type":"text"},"cell_type":"markdown","source":["6. MNIST CLASSIFICATION - OPTIONAL\n","\n","The presented in section 5. approach can be transfered to any type of FFN with ReLU activation. In this section we present a simple MNIST classification task on missing data. "]},{"metadata":{"id":"dX-UG1thhZDq","colab_type":"text"},"cell_type":"markdown","source":["6.1. Set up hyperparameters"]},{"metadata":{"id":"O37pYRsacClc","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import Imputer\n","# Training Parameters\n","learning_rate = 0.001\n","n_epochs = 40\n","batch_size = 64\n","\n","# Network Parameters\n","num_hidden_1 = 256  # 1st layer num features\n","num_hidden_2 = 128  # 256 # 2nd layer num features\n","num_hidden_3 = 64  # 128 # 3nd layer num features (the latent dim)\n","num_out = 10\n","num_input = 784  # MNIST data input (img shape: 28*28)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"52YS9jk21cxv","colab_type":"text"},"cell_type":"markdown","source":["6.2. The accuracy function"]},{"metadata":{"id":"gKx8hoe0jbBQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def accuracy(y_true, y_pred):\n","    assert len(y_true) == len(y_pred)\n","    return np.sum(y_true==y_pred)/len(y_true)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xWmE-wiK1hf5","colab_type":"text"},"cell_type":"markdown","source":["6.3. Set up a classical MNIST classification model. \n","\n","Ex. 13: Build a MNIST classification model. Use 3 hidden layers with ReLU activation on the first one and sigmoid activation on the subsequent layers. The dimensions of the hidden layers are given in section 6.1. Note that this is equal to the architecture of the encoder in sections 2. and 4.  \n","\n","Define the loss and optimization operation. Create and fit the sklearn Imputer model. "]},{"metadata":{"id":"3c6In4H8hhFd","colab_type":"code","colab":{}},"cell_type":"code","source":["X = tf.placeholder(tf.float32, [None, num_input], name=\"P1\" )\n","y = tf.placeholder(tf.int32, [None], name=\"P2\")\n","\n","labels = tf.one_hot(y, num_out)\n","\n","from tensorflow.keras.layers import Dense\n","\n","???\n","pred = ???\n","\n","loss = ???\n","optimizer_op = ???\n","\n","# create an imputer from sklearn, which replace the missing values with the mean.\n","imp = ???\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EmMlFQb62im5","colab_type":"text"},"cell_type":"markdown","source":["6.4. Train the classfication model, imputing the missing values in the input using the `SimpleImputer`."]},{"metadata":{"id":"aE8yB7jZi1fv","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(sess, x_train, y_train, x_valid, y_valid, batch_size, n_epochs, imp):\n","    for epoch in range(1, n_epochs + 1):\n","        #train\n","        n_batches = x_train.shape[0] // batch_size\n","        train_loss = []\n","        for iteration in tqdm(range(n_batches)):\n","            \n","            # get the train batch\n","            batch_x = x_train[iteration * batch_size : (iteration+1) * batch_size, :]\n","            batch_y = y_train[iteration * batch_size : (iteration+1) * batch_size]\n","            batch_x = imp.transform(batch_x)\n","            _, l = sess.run([optimizer_op, loss],  feed_dict={X: batch_x, y:batch_y})\n","            train_loss.append(l)\n","    \n","        #valid\n","        valid_n_batches = x_valid.shape[0]//batch_size\n","        valid_loss = []\n","        for iteration in range(valid_n_batches):\n","            batch_valid_x = x_valid[iteration * batch_size : (iteration+1) * batch_size, :]\n","            batch_valid_y = y_valid[iteration * batch_size : (iteration+1) * batch_size]\n","            batch_valid_x = imp.transform(batch_valid_x)\n","            valid_loss.append(sess.run(loss,  feed_dict={X:batch_valid_x, y:batch_valid_y}))\n","        print('Step %i: Loss mean: %.8f Valid loss mean: %.8f' % (epoch, np.mean(train_loss), np.mean(valid_loss)))\n","        sys.stdout.flush()\n","        #shuffle the train dataset (dummy)\n","        idx = np.random.choice(len(x_train),len(x_train),replace=False)\n","        x_train = x_train[idx]\n","        y_train = y_train[idx]\n","    print('\\n==========================')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zmz5j7yn26JB","colab_type":"text"},"cell_type":"markdown","source":["6.5 Evaluate the model"]},{"metadata":{"id":"0IZv43dHkDKU","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import sys\n","from tqdm import tqdm\n","\n","def test(sess, x_test, y_test, batch_size, imp):\n","    train_loss = []\n","    acc = []\n","    # test\n","    n_batches = x_test.shape[0] // batch_size\n","    for iteration in tqdm(range(n_batches)):\n","        #print(\"\\r{}% \".format(100 * (iteration + 1) // n_batches), end=\"\")\n","        batch_x = x_test[iteration * batch_size : (iteration+1)*batch_size, :]\n","        batch_y = y_test[iteration * batch_size : (iteration+1)*batch_size]\n","        batch_x = imp.transform(batch_x)\n","        l, y_p = sess.run([loss,tf.nn.softmax(pred)], feed_dict={X:batch_x, y:batch_y})\n","        train_loss.append(l)\n","        acc.append(accuracy(batch_y, np.argmax(y_p,axis=1)))\n","        \n","   \n","    print('{}CROSS-ENTROPY for the test dataset: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['bold'], np.mean(train_loss), transform_write['end']))\n","    print('{}ACCURACY for the test dataset {}{:0.5f}{}'.format(transform_write['italic'], transform_write['red'] + transform_write['bold'],np.mean(acc), transform_write['end']))\n","   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"O6z3w-k73HkR","colab_type":"text"},"cell_type":"markdown","source":["6.6. Run the code:"]},{"metadata":{"id":"Qxv0DAsGlHJn","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    sys.stdout.flush()\n","    train(sess, np.copy(train_data), y_train, valid_data, y_valid, batch_size,n_epochs, imp)\n","    sys.stdout.flush()\n","    test(sess, np.copy(test_data), y_test, batch_size, imp)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PVLrFMRL3WNt","colab_type":"text"},"cell_type":"markdown","source":["6.7. Set up the model using the first layer from section 5.  \n","\n","Ex. 14: Change the first layer to the layer defined in section 5. Other model parameters follow the definitions in 6.3"]},{"metadata":{"id":"ikZMhBB03UVh","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","X = tf.placeholder(tf.float32, [None, num_input], name=\"P1\" )\n","y = tf.placeholder(tf.int32, [None], name=\"P2\")\n","\n","\n","labels = tf.one_hot(y, num_out)\n","\n","# means, covs - means and covs of the GMM \n","covs = tf.abs(covs)\n","p = tf.abs(p)\n","p = tf.div(p, tf.reduce_sum(p, axis=0))\n","\n","???\n","pred = ???\n","\n","#pred_soft = tf.nn.softmax(pred)\n","\n","loss = ???\n","optimizer_op = ???\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x67SSULb4EU4","colab_type":"text"},"cell_type":"markdown","source":["6.8 Train the model."]},{"metadata":{"id":"XwkVaoSUxlW_","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def train(sess, x_train, y_train, x_valid, y_valid, batch_size, n_epochs):\n","    for epoch in range(1, n_epochs + 1):\n","        #train\n","        n_batches = x_train.shape[0] // batch_size\n","        train_loss = []\n","        for iteration in tqdm(range(n_batches)):\n","            # get the train batch\n","            batch_x = x_train[iteration * batch_size : (iteration+1) * batch_size, :]\n","            batch_y = y_train[iteration * batch_size : (iteration+1) * batch_size]\n","            #print(batch_x, batch_y)\n","            _, l = sess.run([optimizer_op, loss],  feed_dict={X: batch_x, y:batch_y})\n","            train_loss.append(l)\n","        #valid\n","        valid_n_batches = x_valid.shape[0]//batch_size\n","        valid_loss = []\n","        for iteration in range(valid_n_batches):\n","            batch_valid_x = x_valid[iteration * batch_size : (iteration+1) * batch_size, :]\n","            batch_valid_y = y_valid[iteration * batch_size : (iteration+1) * batch_size]\n","           \n","            valid_loss.append(sess.run(loss,  feed_dict={X:batch_valid_x, y:batch_valid_y}))\n","        print('Step %i: Loss mean: %.8f Valid loss mean: %.8f' % (epoch, np.mean(train_loss), np.mean(valid_loss)))\n","        sys.stdout.flush()\n","        idx = np.random.choice(len(x_train),len(x_train),replace=False)\n","        x_train = x_train[idx]\n","        y_train = y_train[idx]\n","\n","    print('\\n==========================')\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"cAme28gM4IVZ","colab_type":"text"},"cell_type":"markdown","source":["6.9. Evaluate the model"]},{"metadata":{"id":"so9Vu6zL4HsZ","colab_type":"code","colab":{}},"cell_type":"code","source":["def test(sess, x_test, y_test, batch_size):\n","    train_loss = []\n","    acc = []\n","    # test\n","    n_batches = x_test.shape[0] // batch_size\n","    for iteration in tqdm(range(n_batches)):\n","        #print(\"\\r{}% \".format(100 * (iteration + 1) // n_batches), end=\"\")\n","        batch_x = x_test[iteration * batch_size : (iteration+1)*batch_size, :]\n","        batch_y = y_test[iteration * batch_size : (iteration+1)*batch_size]\n","        l, y_p = sess.run([loss,tf.nn.softmax(pred)], feed_dict={X:batch_x, y:batch_y})\n","        train_loss.append(l)\n","        acc.append(accuracy(batch_y, np.argmax(y_p,axis=1)))\n","        \n","   \n","    print('{}CROSS-ENTROPY for the test dataset: {}{:0.5f}{}'.format(transform_write['italic'], transform_write['bold'], np.mean(train_loss), transform_write['end']))\n","    print('{}ACCURACY for the test dataset {}{:0.5f}{}'.format(transform_write['italic'], transform_write['red'] + transform_write['bold'],np.mean(acc), transform_write['end']))\n","   "],"execution_count":0,"outputs":[]},{"metadata":{"id":"FJY1BTHj4XQR","colab_type":"text"},"cell_type":"markdown","source":["6.10: Run the code:"]},{"metadata":{"id":"UWeS-s0K1ohg","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    sys.stdout.flush()\n","    train(sess, np.copy(train_data), y_train, valid_data, y_valid, batch_size, n_epochs)\n","    sys.stdout.flush()\n","    test(sess, np.copy(test_data), y_test, batch_size)"],"execution_count":0,"outputs":[]}]}